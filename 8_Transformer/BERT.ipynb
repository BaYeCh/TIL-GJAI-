{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f87b1229",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = 'C:/pytest/'\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5de1f2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12261453",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from transformers import TFBertModel, BertTokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "290c2a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3bf8545",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graphs(history, string):\n",
    "    plt.plot(history.history[string])\n",
    "    plt.plot(history.history['val_'+string], '')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(string)\n",
    "    plt.legend([string, 'val_'+string])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc0fafb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1111)\n",
    "np.random.seed(1111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01206571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT는 일반 딥러닝과 다르게 TRANSFORMER의 구조를 가짐\n",
    "# 0과 1로 해석해야 하는 각각의 상황에 대한 가중치 값을 가짐\n",
    "# 두 값에 대한 가중치 모두를 가져와야 함\n",
    "CLASS_NUMBER = 2 \n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 2\n",
    "VALID_SPLIT = 0.2\n",
    "MAX_LEN = 40\n",
    "BERT_CKPT = 'C:/pytest/data/KOR/BERT/bert_ckpt/'\n",
    "DATA_IN_PATH = 'C:/pytest/data/KOR/naver_movie/data_in/'\n",
    "DATA_OUT_PATH = 'C:/pytest/data/KOR/BERT/data_out/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c198c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def listToString(listdata):\n",
    "    result = 'id\\tdocument\\tlabel\\n'\n",
    "    for data_each in listdata:\n",
    "        if data_each:\n",
    "            result += data_each[0] + '\\t' + data_each[1] +'\\t'+ data_each[2] + '\\n'\n",
    "    return result\n",
    "\n",
    "def read_data(filename, encoding = 'cp949', start = 0):\n",
    "    with open(filename, 'r', encoding= encoding) as f:\n",
    "        data = [line.split('\\t') for line in f.read().splitlines()]\n",
    "        data = data[start :]\n",
    "    return data\n",
    "\n",
    "def write_data(data, filename, encoding = 'cp949'):\n",
    "    with open(filename, 'w', encoding= encoding) as f:\n",
    "        f.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6cfd64ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ratings = read_data(os.path.join(DATA_IN_PATH, 'ratings_utf8_small.txt'), encoding = 'utf-8', start = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ec7af900",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "ratings_train, ratings_test = train_test_split(data_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "623a6ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_train = listToString(ratings_train)\n",
    "ratings_test = listToString(ratings_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "93bf22c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_data(ratings_train, os.path.join(DATA_IN_PATH,'ratings_train.txt'), encoding = 'utf-8')\n",
    "write_data(ratings_test, os.path.join(DATA_IN_PATH,'ratings_test.txt'), encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a7bdbdf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1364440</td>\n",
       "      <td>내 인생의 영화</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7216894</td>\n",
       "      <td>각본은 인터넷 찌라시 수준 연기는 허접 배우들 얼굴은 부담</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9461185</td>\n",
       "      <td>제가 좋아하는영화중 진짜찐짜좋아하는영화... :)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9738357</td>\n",
       "      <td>솔직히..땡칠이는 너무 날로 먹는거 아냐?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9932738</td>\n",
       "      <td>좋은 영화. 다만 엔딩에서 주인공이 배심원들을 상대로 늘어놓는 일장연설이 너무 작위...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                           document  label\n",
       "0  1364440                                           내 인생의 영화      1\n",
       "1  7216894                   각본은 인터넷 찌라시 수준 연기는 허접 배우들 얼굴은 부담      0\n",
       "2  9461185                        제가 좋아하는영화중 진짜찐짜좋아하는영화... :)      1\n",
       "3  9738357                            솔직히..땡칠이는 너무 날로 먹는거 아냐?      0\n",
       "4  9932738  좋은 영화. 다만 엔딩에서 주인공이 배심원들을 상대로 늘어놓는 일장연설이 너무 작위...      1"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_TRAIN_PATH = os.path.join(DATA_IN_PATH, 'ratings_train.txt')\n",
    "DATA_TEST_PATH = os.path.join(DATA_IN_PATH, 'ratings_test.txt')\n",
    "\n",
    "# quoting = 3 본문 안에 있는 따옴표 처리\n",
    "train_data = pd.read_csv(DATA_TRAIN_PATH, header=0, delimiter= '\\t', quoting= 3)\n",
    "train_data = train_data.dropna()\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d8d96f7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "959e054af26c47e5954631a2e2abf398",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/996k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:123: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\pytest\\data\\KOR\\BERT\\bert_ckpt\\tokenizer. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b578ebc6ed654e3199a2be029d40b029",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a69f0cb8e8140249f1074a59ab11614",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased',\n",
    "                                         cache_dir = os.path.join(BERT_CKPT, 'tokenizer'), do_lower_case = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "07030657",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(DATA_OUT_PATH+'bert_tokenizer.pickle', 'wb') as file:\n",
    "    pickle.dump(tokenizer, file, protocol= pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4f8ae86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 9521, 118741, 35506, 24982, 48549, 117, 9321, 118610, 119081, 48345, 119, 102]\n",
      "['[ C L S ]', '안', '# # 녕', '# # 하', '# # 세', '# # 요', ',', '반', '# # 갑', '# # 습', '# # 니 다', '.', '[ S E P ]']\n"
     ]
    }
   ],
   "source": [
    "test_sentence = '안녕하세요, 반갑습니다.'\n",
    "\n",
    "encode = tokenizer.encode(test_sentence)\n",
    "token_print = [tokenizer.decode(token) for token in encode]\n",
    "\n",
    "print(encode)\n",
    "print(token_print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5f4be40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 9521, 118741, 35506, 24982, 48549, 117, 9321, 118610, 119081, 48345, 102]\n",
      "[CLS] 안녕하세요, 반갑습니다 [SEP]\n",
      "[101, 31178, 11356, 102]\n",
      "[CLS] Hello world [SEP]\n"
     ]
    }
   ],
   "source": [
    "# BERT 토크나이저 연습 2\n",
    "kor_encode = tokenizer.encode('안녕하세요, 반갑습니다')\n",
    "eng_encode = tokenizer.encode('Hello world')\n",
    "kor_decode = tokenizer.decode(kor_encode)\n",
    "eng_decode = tokenizer.decode(eng_encode)\n",
    "\n",
    "print(kor_encode,kor_decode, eng_encode, eng_decode,sep ='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "94df5b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.all_special_tokens)\n",
    "# [UNK] : unkown 모르는 토큰에 대해서도 처리 하겠다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "566ce655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용자 정의 BERT 토크나이저 함수\n",
    "def bert_tokenizer(sent, MAX_LEN):\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        text= sent,\n",
    "        add_special_tokens = True,\n",
    "        max_length = MAX_LEN,\n",
    "        padding = 'max_length', # 패딩 적용에 사용할 문장의 최대 길이\n",
    "        truncation = True,\n",
    "        return_attention_mask = True\n",
    "    )\n",
    "    input_id = encoded_dict['input_ids'] # 각 토큰을 인덱스로 변환\n",
    "    attention_mask = encoded_dict['attention_mask'] # 어텐션 마스크 생성\n",
    "    # 문장이 1개일 경우 0 , 2개일 경우 0과 1로 구분하여 생성\n",
    "    token_type_id = encoded_dict['token_type_ids']\n",
    "    # padding 0 이 있는 부분은 0으로 지정하여 이 부분에는 어텐션이 일어나지 않게 함\n",
    "    # token_type_id : 문장이 2개일 경우 Answer 부분은 0, Question 부분은 1로 구분\n",
    "    # 현재는 모든 문장이 0만 가지고 있음\n",
    "    return input_id, attention_mask, token_type_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ec30b08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 375/375 [00:00<00:00, 3917.38it/s]\n"
     ]
    }
   ],
   "source": [
    "# 훈련 데이터 변환\n",
    "input_ids = []; attention_masks = []; token_type_ids = []; train_data_labels = []\n",
    "for train_sent, train_label in tqdm(zip(train_data['document'], train_data['label']), total = len(train_data)):\n",
    "    try:\n",
    "        input_id, attention_mask, token_type_id = bert_tokenizer(train_sent, MAX_LEN)\n",
    "        \n",
    "        # 문장별 처리\n",
    "        input_ids.append(input_id)\n",
    "        attention_masks.append(attention_mask)\n",
    "        token_type_ids.append(token_type_id)\n",
    "        train_data_labels.append(train_label)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(train_sent)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3a7183d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375 375\n"
     ]
    }
   ],
   "source": [
    "train_movie_input_ids = np.array(input_ids, dtype = int)\n",
    "train_movie_attention_masks = np.array(attention_masks, dtype = int)\n",
    "train_movie_type_ids = np.array(token_type_ids, dtype = int)\n",
    "train_movie_inputs = (train_movie_input_ids, train_movie_attention_masks, train_movie_type_ids)\n",
    "\n",
    "train_data_labels = np.asarray(train_data_labels, dtype = np.int32)\n",
    "print(len(train_movie_input_ids), len(train_data_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "78a2a2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   101   8844  40419  10892   9640  21876  82881   9727  17342  14040\n",
      "   9460  54867   9568  46216   9968 119205  84703  27023   9551 118654\n",
      "  10892   9365 105462    102      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0]\n",
      "[CLS] 각본은 인터넷 찌라시 수준 연기는 허접 배우들 얼굴은 부담 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "input_id = train_movie_input_ids[1] # 인덱스로 변환된 토큰\n",
    "attention_mask = train_movie_attention_masks[1] # 어떤 단어(토큰)을 어텐션 시킬지 여부\n",
    "token_type_id = train_movie_type_ids[1] # 현재는 모든 문장이 0\n",
    "\n",
    "print(input_id)\n",
    "print(attention_mask)\n",
    "# padding 0이 들어간 부분만 attention_mask에서 0이 입력되어\n",
    "# 이 부분에는 어텐션이 일어나지 않게 함\n",
    "print(token_type_id)\n",
    "print(tokenizer.decode(input_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d6b60405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT 분류기 사용자 정의\n",
    "class TFBertClassifier(tf.keras.Model):\n",
    "    '''\n",
    "        outputs[0]~[3]의 내용:\n",
    "        0: sequence output = 각 문장의 토큰의 은닉상태값. 입력 문장의 각 토큰에 대한 은닉상태(last_hidden_state)\n",
    "        1: pooler_output = 전체 문장의 은닉상태값. 활성화 함수 적용되어 있음\n",
    "        2: hidden_states = optional. 모델의 각 층과 초기 임베딩 결과에 대한 은닉상태 값\n",
    "        3: attentions = optional. 어텐션 가중치\n",
    "    '''\n",
    "    def __init__(self, model_name, dir_path, num_class):\n",
    "        super().__init__()\n",
    "        self.bert = TFBertModel.from_pretrained(model_name, cache_dir=  dir_path)\n",
    "        self.dropout = tf.keras.layers.Dropout(self.bert.config.hidden_dropout_prob)\n",
    "        # 활성화 함수를 적용하지 않으면 선형으로 결과 출력\n",
    "        # call 함수에 에서 bert의 반환 값인 outputs를 인덱스 [1]번을 사용하면 활성화 함수 사용하지 않아도 됨\n",
    "        # 출력층\n",
    "        self.classifier = tf.keras.layers.Dense(num_class, name = 'classifier', activation = 'softmax',\n",
    "        # Dense층의 초깃값 생성\n",
    "        kernel_initializer = tf.keras.initializers.TruncatedNormal(self.bert.config.initializer_range))\n",
    "        \n",
    "    def call(self, inputs, attention_mask = None, token_type_ids = None, training = False):\n",
    "        # outputs 값 : sequence_output, pooler_output, (hidden_states), (attentions)\n",
    "        outputs = self.bert(inputs, attention_mask = attention_mask, token_type_ids = token_type_ids)\n",
    "        pooler_output = outputs[1]\n",
    "        pooler_output = self.dropout(pooler_output\n",
    "                                     #, training = training\n",
    "                                    )\n",
    "        logits = self.classifier(pooler_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b311c858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d33796874f184a6fb275ef2363db2490",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:123: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\pytest\\data\\KOR\\BERT\\bert_ckpt\\model. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "177d4420f9754928a6b658166d8ec586",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.08G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-multilingual-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-multilingual-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "cls_model = TFBertClassifier(model_name = 'bert-base-multilingual-cased', dir_path = os.path.join(BERT_CKPT, 'model'), num_class = CLASS_NUMBER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "319d8771",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(3e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "cls_model.compile(optimizer=optimizer, loss= loss, metrics = [metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "13c78dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create\n"
     ]
    }
   ],
   "source": [
    "model_name = 'tf2_bert'\n",
    "\n",
    "earlystop_callback = EarlyStopping(monitor= 'val_accuracy', min_delta = 0.0001, patience = 2)\n",
    "\n",
    "checkpoint_path = os.path.join(DATA_OUT_PATH, model_name, 'weights.h5')\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    print('exists')\n",
    "else:\n",
    "    os.makedirs(checkpoint_dir, exist_ok= True)\n",
    "    print('create')\n",
    "\n",
    "cp_callback = ModelCheckpoint(checkpoint_path, monitor= 'val_accuracy', verbose =1, save_best_only= True, save_weights_only= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "167ae0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node 'Adam/Adam/update/mul_1' defined at (most recent call last):\n    File \"C:\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n      \"__main__\", mod_spec)\n    File \"C:\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"C:\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 978, in launch_instance\n      app.start()\n    File \"C:\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"C:\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Anaconda3\\lib\\asyncio\\base_events.py\", line 534, in run_forever\n      self._run_once()\n    File \"C:\\Anaconda3\\lib\\asyncio\\base_events.py\", line 1771, in _run_once\n      handle._run()\n    File \"C:\\Anaconda3\\lib\\asyncio\\events.py\", line 88, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"C:\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"C:\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 387, in do_execute\n      cell_id=cell_id,\n    File \"C:\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2976, in run_cell\n      raw_cell, store_history, silent, shell_futures, cell_id\n    File \"C:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3030, in _run_cell\n      return runner(coro)\n    File \"C:\\Anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 78, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3258, in run_cell_async\n      interactivity=interactivity, compiler=compiler, result=result)\n    File \"C:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3473, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"C:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3553, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\광주인공지능사관학교\\AppData\\Local\\Temp\\ipykernel_27272\\1895341038.py\", line 2, in <module>\n      validation_split = VALID_SPLIT, callbacks= [earlystop_callback, cp_callback])\n    File \"C:\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 997, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"C:\\Anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 579, in minimize\n      return self.apply_gradients(grads_and_vars, name=name)\n    File \"C:\\Anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 744, in apply_gradients\n      name=name,\n    File \"C:\\Anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 801, in _distributed_apply\n      group=False,\n    File \"C:\\Anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 771, in apply_grad_to_update_var\n      grad.values, var, grad.indices, **apply_kwargs\n    File \"C:\\Anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 1454, in _resource_apply_sparse_duplicate_indices\n      summed_grad, handle, unique_indices, **kwargs\n    File \"C:\\Anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py\", line 217, in _resource_apply_sparse\n      m, m * coefficients[\"beta_1_t\"], use_locking=self._use_locking\nNode: 'Adam/Adam/update/mul_1'\nDetected at node 'Adam/Adam/update/mul_1' defined at (most recent call last):\n    File \"C:\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n      \"__main__\", mod_spec)\n    File \"C:\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"C:\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 978, in launch_instance\n      app.start()\n    File \"C:\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"C:\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Anaconda3\\lib\\asyncio\\base_events.py\", line 534, in run_forever\n      self._run_once()\n    File \"C:\\Anaconda3\\lib\\asyncio\\base_events.py\", line 1771, in _run_once\n      handle._run()\n    File \"C:\\Anaconda3\\lib\\asyncio\\events.py\", line 88, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"C:\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"C:\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 387, in do_execute\n      cell_id=cell_id,\n    File \"C:\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2976, in run_cell\n      raw_cell, store_history, silent, shell_futures, cell_id\n    File \"C:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3030, in _run_cell\n      return runner(coro)\n    File \"C:\\Anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 78, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3258, in run_cell_async\n      interactivity=interactivity, compiler=compiler, result=result)\n    File \"C:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3473, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"C:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3553, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\광주인공지능사관학교\\AppData\\Local\\Temp\\ipykernel_27272\\1895341038.py\", line 2, in <module>\n      validation_split = VALID_SPLIT, callbacks= [earlystop_callback, cp_callback])\n    File \"C:\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 997, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"C:\\Anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 579, in minimize\n      return self.apply_gradients(grads_and_vars, name=name)\n    File \"C:\\Anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 744, in apply_gradients\n      name=name,\n    File \"C:\\Anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 801, in _distributed_apply\n      group=False,\n    File \"C:\\Anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 771, in apply_grad_to_update_var\n      grad.values, var, grad.indices, **apply_kwargs\n    File \"C:\\Anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 1454, in _resource_apply_sparse_duplicate_indices\n      summed_grad, handle, unique_indices, **kwargs\n    File \"C:\\Anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py\", line 217, in _resource_apply_sparse\n      m, m * coefficients[\"beta_1_t\"], use_locking=self._use_locking\nNode: 'Adam/Adam/update/mul_1'\n2 root error(s) found.\n  (0) RESOURCE_EXHAUSTED:  failed to allocate memory\n\t [[{{node Adam/Adam/update/mul_1}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n\t [[gradient_tape/tf_bert_classifier/tf_bert_model/bert/encoder/layer_._7/attention/output/LayerNorm/batchnorm/mul/Shape_1/_324]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n  (1) RESOURCE_EXHAUSTED:  failed to allocate memory\n\t [[{{node Adam/Adam/update/mul_1}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_24727]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_27272\\413592154.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/CPU:0'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     history = cls_model.fit(train_movie_inputs, train_data_labels, epochs= NUM_EPOCHS, batch_size = BATCH_SIZE,\n\u001b[1;32m----> 3\u001b[1;33m                        validation_split = VALID_SPLIT, callbacks= [earlystop_callback, cp_callback])\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 55\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node 'Adam/Adam/update/mul_1' defined at (most recent call last):\n    File \"C:\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n      \"__main__\", mod_spec)\n    File \"C:\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"C:\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 978, in launch_instance\n      app.start()\n    File \"C:\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"C:\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Anaconda3\\lib\\asyncio\\base_events.py\", line 534, in run_forever\n      self._run_once()\n    File \"C:\\Anaconda3\\lib\\asyncio\\base_events.py\", line 1771, in _run_once\n      handle._run()\n    File \"C:\\Anaconda3\\lib\\asyncio\\events.py\", line 88, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"C:\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"C:\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 387, in do_execute\n      cell_id=cell_id,\n    File \"C:\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2976, in run_cell\n      raw_cell, store_history, silent, shell_futures, cell_id\n    File \"C:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3030, in _run_cell\n      return runner(coro)\n    File \"C:\\Anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 78, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3258, in run_cell_async\n      interactivity=interactivity, compiler=compiler, result=result)\n    File \"C:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3473, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"C:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3553, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\광주인공지능사관학교\\AppData\\Local\\Temp\\ipykernel_27272\\1895341038.py\", line 2, in <module>\n      validation_split = VALID_SPLIT, callbacks= [earlystop_callback, cp_callback])\n    File \"C:\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 997, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"C:\\Anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 579, in minimize\n      return self.apply_gradients(grads_and_vars, name=name)\n    File \"C:\\Anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 744, in apply_gradients\n      name=name,\n    File \"C:\\Anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 801, in _distributed_apply\n      group=False,\n    File \"C:\\Anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 771, in apply_grad_to_update_var\n      grad.values, var, grad.indices, **apply_kwargs\n    File \"C:\\Anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 1454, in _resource_apply_sparse_duplicate_indices\n      summed_grad, handle, unique_indices, **kwargs\n    File \"C:\\Anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py\", line 217, in _resource_apply_sparse\n      m, m * coefficients[\"beta_1_t\"], use_locking=self._use_locking\nNode: 'Adam/Adam/update/mul_1'\nDetected at node 'Adam/Adam/update/mul_1' defined at (most recent call last):\n    File \"C:\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n      \"__main__\", mod_spec)\n    File \"C:\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"C:\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 978, in launch_instance\n      app.start()\n    File \"C:\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"C:\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Anaconda3\\lib\\asyncio\\base_events.py\", line 534, in run_forever\n      self._run_once()\n    File \"C:\\Anaconda3\\lib\\asyncio\\base_events.py\", line 1771, in _run_once\n      handle._run()\n    File \"C:\\Anaconda3\\lib\\asyncio\\events.py\", line 88, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"C:\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"C:\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 387, in do_execute\n      cell_id=cell_id,\n    File \"C:\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2976, in run_cell\n      raw_cell, store_history, silent, shell_futures, cell_id\n    File \"C:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3030, in _run_cell\n      return runner(coro)\n    File \"C:\\Anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 78, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3258, in run_cell_async\n      interactivity=interactivity, compiler=compiler, result=result)\n    File \"C:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3473, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"C:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3553, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\광주인공지능사관학교\\AppData\\Local\\Temp\\ipykernel_27272\\1895341038.py\", line 2, in <module>\n      validation_split = VALID_SPLIT, callbacks= [earlystop_callback, cp_callback])\n    File \"C:\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 997, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"C:\\Anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 579, in minimize\n      return self.apply_gradients(grads_and_vars, name=name)\n    File \"C:\\Anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 744, in apply_gradients\n      name=name,\n    File \"C:\\Anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 801, in _distributed_apply\n      group=False,\n    File \"C:\\Anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 771, in apply_grad_to_update_var\n      grad.values, var, grad.indices, **apply_kwargs\n    File \"C:\\Anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 1454, in _resource_apply_sparse_duplicate_indices\n      summed_grad, handle, unique_indices, **kwargs\n    File \"C:\\Anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py\", line 217, in _resource_apply_sparse\n      m, m * coefficients[\"beta_1_t\"], use_locking=self._use_locking\nNode: 'Adam/Adam/update/mul_1'\n2 root error(s) found.\n  (0) RESOURCE_EXHAUSTED:  failed to allocate memory\n\t [[{{node Adam/Adam/update/mul_1}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n\t [[gradient_tape/tf_bert_classifier/tf_bert_model/bert/encoder/layer_._7/attention/output/LayerNorm/batchnorm/mul/Shape_1/_324]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n  (1) RESOURCE_EXHAUSTED:  failed to allocate memory\n\t [[{{node Adam/Adam/update/mul_1}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_24727]"
     ]
    }
   ],
   "source": [
    "history = cls_model.fit(train_movie_inputs, train_data_labels, epochs= NUM_EPOCHS, batch_size = BATCH_SIZE,\n",
    "                       validation_split = VALID_SPLIT, callbacks= [earlystop_callback, cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7b4921",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
