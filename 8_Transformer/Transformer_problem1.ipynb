{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca772b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = \"C:/pytest/data/transformer/\"\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8d2f3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6997b419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('transformer.pickle', 'rb') as f:\n",
    "    loaded_tokenizer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "280537b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('transformer_dict.pickle','rb') as f:\n",
    "    transformer_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5a95c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_pad = transformer_dict['encoder_input_pad']\n",
    "decoder_input_pad = transformer_dict['decoder_input_pad']\n",
    "decoder_target_pad = transformer_dict['decoder_target_pad']\n",
    "sentence_max_length = transformer_dict['sentence_max_length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25a63bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = loaded_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c629ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_INDEX = 0\n",
    "STD_INDEX = 1\n",
    "END_INDEX = 2\n",
    "\n",
    "index_inputs = encoder_input_pad\n",
    "index_outputs = decoder_input_pad\n",
    "index_targets = decoder_target_pad\n",
    "\n",
    "char2idx_dict = word_index\n",
    "idx2char_dict = {y:x for x,y in word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acfd3b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "char2idx_dict['<PAD>'] = 0\n",
    "\n",
    "char2idx_dict['<SOS>'] = char2idx_dict['SOS']\n",
    "del char2idx_dict['SOS']\n",
    "\n",
    "char2idx_dict['<END>'] = char2idx_dict['EOS']\n",
    "del char2idx_dict['EOS']\n",
    "\n",
    "idx2char_dict[0] = '<PAD>'\n",
    "idx2char_dict[1] = '<SOS>'\n",
    "idx2char_dict[2] = '<END>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a249102a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepro_configs = dict({'char2idx': char2idx_dict, 'idx2char': idx2char_dict, \n",
    "                      'vocab_size': len(word_index), 'pad_symbol':'<PAD>','std_symbol':'<SOS>',\n",
    "                      'end_symbol': '<END>'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "154a7fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "char2idx = prepro_configs['char2idx']\n",
    "end_index = prepro_configs['end_symbol']\n",
    "vocab_size = prepro_configs['vocab_size']\n",
    "MAX_SEQUENCE = 25\n",
    "BATCH_SIZE = 2\n",
    "EPOCHS = 30\n",
    "VALID_SPLIT = 0.1\n",
    "model_name = 'transformer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a415e4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "kargs = {'model_name': model_name,\n",
    "         'num_layers': 2,\n",
    "         'd_model': 512, # 단어의 차원 = 임베딩 dimension\n",
    "         'num_heads':8,\n",
    "         'dff': 2048, # 출력층의 노드 수\n",
    "         'input_vocab_size': vocab_size, # 단어 사전의 수\n",
    "         'target_vocab_size': vocab_size, # 단어 사전의 수\n",
    "         'maximum_position_encoding': MAX_SEQUENCE, # 포지션 인코더의 최대 시퀀스 길이\n",
    "         'end_token_idx': char2idx[end_index], # 종료 표지의 인덱스\n",
    "         'rate' : 0.1 # Dropout에 사용되는 비율\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b2568aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    mask = tf.cast(tf.math.equal(seq,0), tf.float32)\n",
    "    return mask[:, tf.newaxis, tf.newaxis,:]\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1- tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask\n",
    "\n",
    "def create_masks(inp, tar):\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "    dec_padding_mask = create_padding_mask(inp)\n",
    "    \n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "    \n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0bfced46",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_padding_mask, look_ahead_mask, dec_padding_mask = create_masks(index_inputs, index_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "683b2f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1/np.power(10000,(2*i//2)/np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:,np.newaxis],\n",
    "                           np.arange(d_model)[np.newaxis,:],\n",
    "                           d_model)\n",
    "    angle_rads[:,0::2] = np.sin(angle_rads[:,0::2])\n",
    "    angle_rads[:,1::2] = np.cos(angle_rads[:,1::2])\n",
    "    \n",
    "    pos_encoding=  angle_rads[np.newaxis,...]\n",
    "    return tf.cast(pos_encoding, dtype = tf.float32)\n",
    "\n",
    "def scaled_dot_product_attention(q,k,v,mask):\n",
    "    matmul_qk = tf.matmul(q,k,transpose_b = True)\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "    \n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis = -1)\n",
    "    output = tf.matmul(attention_weights,v)\n",
    "    \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f588cfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kargs):\n",
    "        super().__init__() # 부모의 기능 가져오기, Layers 클래스의 메서드 가져오기\n",
    "        self.num_heads = kargs['num_heads'] # 어텐션 헤드 수 8\n",
    "        self.d_model = kargs['d_model'] # 단어의 차원 수 512\n",
    "        \n",
    "        # assert구문은 문제 발생 시 알림 역할\n",
    "        assert self.d_model % self.num_heads ==0\n",
    "        # d_model의 차원 수는 헤드의 개수로 나머지 없이 나뉘어야 함\n",
    "        \n",
    "        self.depth = self.d_model // self.num_heads\n",
    "        # 각 헤드에 입력될 벡터의 차원 수를 둘을 나눈 몫으로 결정\n",
    "        \n",
    "        # query, key, value 가중치 레이어 설정\n",
    "        # input 결과를 받을 수 있도록 차원 수를 동일하게\n",
    "        self.wq = tf.keras.layers.Dense(kargs['d_model'])\n",
    "        self.wk = tf.keras.layers.Dense(kargs['d_model'])\n",
    "        self.wv = tf.keras.layers.Dense(kargs['d_model'])\n",
    "        \n",
    "        # 셀프 어텐션 결과를 출력하기 위한 레이어\n",
    "        self.dense = tf.keras.layers.Dense(kargs['d_model'])\n",
    "        \n",
    "    # 각 배치 사이즈마다 데이터가 [seq_len X depth]로 되어 있는 것을\n",
    "    # [num_heads X seq_len X depth]로 변환, 헤드 수 만큼 분리하는 함수\n",
    "    # (depth == d_model == 임베딩 차원)\n",
    "    def split_heads(self, x, batch_size):\n",
    "        # (batch_size, seq_len, depth) -> (batch_size, seq_len, num_heads, depth)\n",
    "        # seq_len는 -1로 표기하여 자동 배정\n",
    "        # 숫자가 기입된 부분의 축을 변환하고 난 뒤 남은 축의 형태는 원래 텐서의\n",
    "        # 총 크기와 같도록 자동으로 결정\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "\n",
    "        # (batch_size, num_heads, seq_len, depth)로 치환\n",
    "        return tf.transpose(x, perm = [0,2,1,3])\n",
    "        \n",
    "    # fit단계(훈련)에서 실행되는 함수\n",
    "    def call(self, v,k,q,mask):\n",
    "        batch_size = tf.shape(q)[0] # batch size를 구함\n",
    "        \n",
    "        # (batch_size, seq_len, d_model)\n",
    "        q = self.wq(q)\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "        \n",
    "        # (batch_size, num_heads, seq_len, depth)\n",
    "        # num_heads 별로 depth(임베딩 차원)를 갖게함\n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "        \n",
    "        # 스케일 내적 어텐션 수행\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(q,k,v,mask)\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm = [0,2,1,3])\n",
    "        \n",
    "        # 4D -> 3D 변환 (batch_size, seq_len, d_model)\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
    "        # 가중합 (어텐션 결과)\n",
    "        \n",
    "        output = self.dense(concat_attention)\n",
    "        \n",
    "        # attention_weigth : softmax를 거친 확률 정보\n",
    "        # 어텐션을 얼마나 적용시킬 것인지에 대한 정보\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5df06cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward_network(**kargs):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(kargs['dff'], activation = 'relu'),\n",
    "        tf.keras.layers.Dense(kargs['d_model'])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1303bd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kargs):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(**kargs)\n",
    "        self.ffn = feed_forward_network(**kargs)\n",
    "        \n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon = 1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon = 1e-6)\n",
    "        \n",
    "        self.dropout1 = tf.keras.layers.Dropout(kargs['rate'])\n",
    "        self.dropout2 = tf.keras.layers.Dropout(kargs['rate'])\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        attn_output, _ = self.mha(x,x,x,mask)\n",
    "        \n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layernorm1(x+attn_output)\n",
    "        \n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29283760",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kargs):\n",
    "        super().__init__()\n",
    "        self.mha1 = MultiHeadAttention(**kargs)\n",
    "        self.mha2 = MultiHeadAttention(**kargs)\n",
    "        self.ffn = feed_forward_network(**kargs)\n",
    "        \n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon = 1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon = 1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon = 1e-6)\n",
    "        \n",
    "        self.dropout1 = tf.keras.layers.Dropout(kargs['rate'])\n",
    "        self.dropout2 = tf.keras.layers.Dropout(kargs['rate'])\n",
    "        self.dropout3 = tf.keras.layers.Dropout(kargs['rate'])\n",
    "        \n",
    "    def call(self, x, enc_output, look_ahead_mask, padding_mask):\n",
    "        attn1, attn_weights_block1 = self.mha1(x,x,x,look_ahead_mask)\n",
    "        attn1 = self.dropout1(attn1)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "        \n",
    "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)\n",
    "        attn2 = self.dropout2(attn2)\n",
    "        out2 = self.layernorm2(attn2 + out1)\n",
    "        \n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output)\n",
    "        out3 = self.layernorm3(ffn_output + out2)\n",
    "        \n",
    "        return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc2c3cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kargs):\n",
    "        super().__init__()\n",
    "        self.d_model = kargs['d_model']\n",
    "        self.num_layers = kargs['num_layers']\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(input_dim = kargs['input_vocab_size'],output_dim = self.d_model)\n",
    "        \n",
    "        self.pos_encoding = positional_encoding(position= kargs['maximum_position_encoding'], d_model = self.d_model)\n",
    "        \n",
    "        self.enc_layers = [EncoderLayer(**kargs) for _ in range(self.num_layers)]\n",
    "        \n",
    "        self.dropout = tf.keras.layers.Dropout(kargs['rate'])\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:,:seq_len, :]\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, mask)\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc189133",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kargs):\n",
    "        super().__init__()\n",
    "        self.d_model = kargs['d_model']\n",
    "        self.num_layers = kargs['num_layers']\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(input_dim = kargs['target_vocab_size'], output_dim = self.d_model)\n",
    "        \n",
    "        self.pos_encoding = positional_encoding(position= kargs['maximum_position_encoding'], d_model = self.d_model)\n",
    "        \n",
    "        self.dec_layers = [DecoderLayer(**kargs) for _ in range(self.num_layers)]\n",
    "        \n",
    "        self.dropout = tf.keras.layers.Dropout(kargs['rate'])\n",
    "        \n",
    "    def call(self, x, enc_output, look_ahead_mask, padding_mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:,:seq_len,:]\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, look_ahead_mask, padding_mask)\n",
    "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa1874bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, **kargs):\n",
    "        super().__init__(name = kargs['model_name'])\n",
    "        self.end_token_idx = kargs['end_token_idx']\n",
    "        \n",
    "        self.encoder = Encoder(**kargs)\n",
    "        self.decoder = Decoder(**kargs)\n",
    "        \n",
    "        self.final_layer = tf.keras.layers.Dense(kargs['target_vocab_size'])\n",
    "        \n",
    "    def call(self, x):\n",
    "        inp, tar = x\n",
    "        \n",
    "        enc_padding_mask, look_ahead_mask, dec_padding_mask = create_masks(inp, tar)\n",
    "        \n",
    "        enc_output = self.encoder(inp, enc_padding_mask)\n",
    "        \n",
    "        dec_output, _ = self.decoder(tar, enc_output, look_ahead_mask, dec_padding_mask)\n",
    "        \n",
    "        final_output = self.final_layer(dec_output)\n",
    "        \n",
    "        return final_output\n",
    "    \n",
    "    def inference(self, x):\n",
    "        inp = x\n",
    "        tar = tf.expand_dims([STD_INDEX], axis =0)\n",
    "        enc_padding_mask, look_ahead_mask, dec_padding_mask = create_masks(inp, tar)\n",
    "        \n",
    "        enc_output = self.encoder(inp, enc_padding_mask)\n",
    "        \n",
    "        predict_tokens = list()\n",
    "        \n",
    "        for t in range(0, MAX_SEQUENCE):\n",
    "            dec_output, _ = self.decoder(tar, enc_output, look_ahead_mask, dec_padding_mask)\n",
    "            final_output = self.final_layer(dec_output)\n",
    "            outputs=  tf.argmax(final_output, axis = -1).numpy()\n",
    "            pred_token = outputs[0][-1]\n",
    "            \n",
    "            if pred_token == self.end_token_idx:\n",
    "                break\n",
    "            predict_tokens.append(pred_token)\n",
    "            tar = tf.expand_dims([STD_INDEX] + predict_tokens, axis = 0)\n",
    "            _, look_ahead_mask, dec_padding_mask = create_masks(inp, tar)\n",
    "            \n",
    "        return predict_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4780d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True)\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ccb1aa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real,0))\n",
    "    \n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype = loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "def accuracy(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real,0))\n",
    "    \n",
    "    mask = tf.expand_dims(tf.cast(mask, dtype = pred.dtype), axis = -1)\n",
    "    pred *= mask\n",
    "    acc  = train_accuracy(real, pred)\n",
    "    \n",
    "    return tf.reduce_mean(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "74aa78d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(**kargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fc10aef3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x1962877c5c8>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('weights_cp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9af07d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = pd.Series(['가스불 끈 것을 까먹은 것은 아니겠죠?'],name = 'sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "41d3624b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    가스불 끈 것을 까먹은 것은 아니겠죠?\n",
       "Name: sentence, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e60550b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1_sequencing = loaded_tokenizer.texts_to_sequences(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ec159a40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[12]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1_sequencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5565d7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "text1_padding = pad_sequences(text1_sequencing, maxlen = sentence_max_length, padding = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e7b4629c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12,  0,  0,  0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1_padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "acaeedcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1_inferencing =[model.inference(text1_padding)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a4b4dd09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['다시 새로 사는 게 마음 편해요']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_tokenizer.sequences_to_texts(text1_inferencing)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
