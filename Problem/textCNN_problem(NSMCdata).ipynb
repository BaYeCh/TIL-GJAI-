{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78dcdba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = \"C:/pytest/\"\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d97dbe89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename, encoding = 'cp949',start = 1):\n",
    "    with open(filename, 'r', encoding = encoding) as f:\n",
    "        data = [line.split('\\t') for line in f.read().splitlines()]\n",
    "        data = data[start:]\n",
    "    return data\n",
    "def write_data(data, filename, encoding = 'cp949'):\n",
    "    with open(filename, 'w', encoding = encoding) as f:\n",
    "        f.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a248b713",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_data('ratings_morphed.txt', encoding ='cp949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2215ea66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00082c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text = [line[1] for line in data]\n",
    "data_senti = [line[2] for line in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4469fc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_X, test_X, train_y, test_y = train_test_split(data_text, data_senti, stratify = data_senti)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb92fa6",
   "metadata": {},
   "source": [
    "# Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00f9b66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95986932",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 10000 # 데이터셋에서 가장 빈도 높은 10,000 개의 단어만 사용\n",
    "                  # n-1개의 단어를 가져오는 데 이는 padding을 위한 단어 하나를 남겨 둠\n",
    "maxlen = 20 # 20개 이후의 단어는 버려서 각 문장의 길이를 고정\n",
    "            # 20보다 길면 뒤를 자르고, 작으면 0으로 채움\n",
    "embedding_dim = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f188ac3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_words) # 상위빈도 10,000개 단어를 추려내는 Tokenizer 객체 생성\n",
    "tokenizer.fit_on_texts(train_X) # 전체 단어를 대상으로 인덱스 구축(빈도 기준)\n",
    "word_index = tokenizer.word_index # 단어 인덱스 가져오기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eefd179",
   "metadata": {},
   "source": [
    "### Data Sequencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7dfd72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스틀 숫자로 변환\n",
    "train_X_sequencing = tokenizer.texts_to_sequences(train_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3405cc4b",
   "metadata": {},
   "source": [
    "# Data Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d53cb29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_padding = pad_sequences(train_X_sequencing, maxlen= maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1abae4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = np.array(train_y).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5353d0",
   "metadata": {},
   "source": [
    "# 모델 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee03fe76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "76d1acde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_11 (Embedding)    (None, 20, 200)           2000000   \n",
      "                                                                 \n",
      " conv1d_11 (Conv1D)          (None, 18, 64)            38464     \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 1152)              0         \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 1152)              0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 1)                 1153      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,039,617\n",
      "Trainable params: 2,039,617\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Embedding(input_dim = max_words, output_dim = embedding_dim, input_length = maxlen))\n",
    "\n",
    "model.add(layers.Conv1D(64, kernel_size = 3,activation = 'relu'))\n",
    "# model.add(layers.MaxPooling1D(pool_size = 2)) # maxpooling + lstm : 82.8 / maxpooling+flatten : 84.1\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dropout(0.4)) # conv + dropout + lstm + dropout : 84.5\n",
    "\n",
    "# model.add(layers.LSTM(64)) # 83.6\n",
    "\n",
    "# model.add(layers.Dropout(0.4))\n",
    "# model.add(layers.Dense(32, activation = 'relu'))\n",
    "model.add(layers.Dense(1, activation = 'sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "99f4818b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3242/3242 [==============================] - 20s 6ms/step - loss: 0.4128 - acc: 0.8103 - val_loss: 0.3775 - val_acc: 0.8322\n",
      "Epoch 2/10\n",
      "3242/3242 [==============================] - 20s 6ms/step - loss: 0.3628 - acc: 0.8435 - val_loss: 0.3649 - val_acc: 0.8403\n",
      "Epoch 3/10\n",
      "3242/3242 [==============================] - 20s 6ms/step - loss: 0.3463 - acc: 0.8539 - val_loss: 0.4102 - val_acc: 0.8206\n",
      "Epoch 4/10\n",
      "3242/3242 [==============================] - 20s 6ms/step - loss: 0.3332 - acc: 0.8617 - val_loss: 0.3971 - val_acc: 0.8301\n",
      "Epoch 5/10\n",
      "3242/3242 [==============================] - 20s 6ms/step - loss: 0.3194 - acc: 0.8694 - val_loss: 0.3753 - val_acc: 0.8398\n",
      "Epoch 6/10\n",
      "3242/3242 [==============================] - 21s 6ms/step - loss: 0.3056 - acc: 0.8764 - val_loss: 0.3838 - val_acc: 0.8395\n",
      "Epoch 7/10\n",
      "3242/3242 [==============================] - 20s 6ms/step - loss: 0.2902 - acc: 0.8843 - val_loss: 0.3975 - val_acc: 0.8358\n",
      "Epoch 8/10\n",
      "3242/3242 [==============================] - 21s 6ms/step - loss: 0.2818 - acc: 0.8894 - val_loss: 0.4166 - val_acc: 0.8359\n",
      "Epoch 9/10\n",
      "3242/3242 [==============================] - 21s 6ms/step - loss: 0.2730 - acc: 0.8936 - val_loss: 0.4244 - val_acc: 0.8353\n",
      "Epoch 10/10\n",
      "3242/3242 [==============================] - 21s 6ms/step - loss: 0.2612 - acc: 0.8992 - val_loss: 0.4512 - val_acc: 0.8147\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss= 'binary_crossentropy', optimizer = 'rmsprop', metrics = ['acc'])\n",
    "with tf.device('/CPU:0'):\n",
    "    history = model.fit(train_X_padding, train_y, epochs=10, batch_size=32, validation_split=0.3)\n",
    "history_dict = history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0214fe23",
   "metadata": {},
   "source": [
    "# 테스트 데이터 처리\n",
    "+ 앞선 훈련 데이터 처리한 것과 동일한 과정을 거쳐야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6332b5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequencing\n",
    "test_X_sequencing = tokenizer.texts_to_sequences(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fdd0ee63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding\n",
    "test_X_padding = pad_sequences(test_X_sequencing, maxlen = maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8013e651",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y = np.asarray(test_y).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e98a974a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1544/1544 [==============================] - 4s 3ms/step - loss: 0.4502 - acc: 0.8146\n",
      "[0.450157105922699, 0.8145980834960938]\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "    test_eval = model.evaluate(test_X_padding, test_y)\n",
    "print(test_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ac5132",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
