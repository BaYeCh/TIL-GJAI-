{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "609d103e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = \"C:/pytest/\"\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcf64f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad971f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_train = pd.read_csv('data1_train.csv',encoding= 'cp949')\n",
    "data1_test = pd.read_csv('data1_test.csv',encoding= 'cp949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "094fa1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_train_X_morphed = data1_train['텍스트']\n",
    "data1_train_y = data1_train['분류1_상위']\n",
    "data1_test_X_morphed = data1_train['텍스트']\n",
    "data1_test_y = data1_train['분류1_상위']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "852cfe83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa0646e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 10000\n",
    "maxlen = 30\n",
    "embedding_dim = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "112232ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(num_words = max_words)\n",
    "tokenizer.fit_on_texts(data1_train_X_morphed)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e352379",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f0878df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequencing \n",
    "data1_train_X_sequencing = tokenizer.texts_to_sequences(data1_train_X_morphed)\n",
    "# padding\n",
    "data1_train_X_padding = pad_sequences(data1_train_X_sequencing, maxlen = maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cad0f7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "e = LabelEncoder()\n",
    "e.fit(data1_train_y)\n",
    "data1_train_y_labeling = e.transform(data1_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "970babd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['IT_과학' '경제' '국제' '문화' '미분류' '사회' '스포츠' '정치' '지역']\n"
     ]
    }
   ],
   "source": [
    "print(e.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27c352ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(sequences, dimension):\n",
    "  results = np.zeros((len(sequences), dimension))\n",
    "  for i, sequence in enumerate(sequences):\n",
    "    results[i, sequence] = 1\n",
    "  return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "315864cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_train_y_1hot = to_one_hot(data1_train_y_labeling, len(e.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dcda4368",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_number = len(e.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a460815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 30, 200)           2000000   \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 28, 64)            38464     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 28, 64)            0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                33024     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 9)                 297       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,073,865\n",
      "Trainable params: 2,073,865\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers, regularizers\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(input_dim = max_words, output_dim = embedding_dim, input_length = maxlen))\n",
    "\n",
    "model.add(layers.Conv1D(64, kernel_size = 3,activation = 'relu'))\n",
    "# model.add(layers.MaxPooling1D(pool_size = 2)) # maxpooling + lstm : 82.8 / maxpooling+flatten : 84.1\n",
    "# model.add(layers.Flatten())\n",
    "model.add(layers.Dropout(0.4)) # conv + dropout + lstm + dropout : 84.5\n",
    "\n",
    "model.add(layers.LSTM(64)) # 83.6\n",
    "\n",
    "model.add(layers.Dropout(0.4))\n",
    "model.add(layers.Dense(32, activation = 'relu'))\n",
    "model.add(layers.Dense(class_number, activation = 'softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "674ab93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "149/149 [==============================] - 4s 18ms/step - loss: 1.4967 - acc: 0.4651 - val_loss: 1.2524 - val_acc: 0.5515\n",
      "Epoch 2/10\n",
      "149/149 [==============================] - 3s 18ms/step - loss: 1.0154 - acc: 0.6262 - val_loss: 1.1048 - val_acc: 0.6030\n",
      "Epoch 3/10\n",
      "149/149 [==============================] - 2s 15ms/step - loss: 0.8124 - acc: 0.7087 - val_loss: 1.0764 - val_acc: 0.6398\n",
      "Epoch 4/10\n",
      "149/149 [==============================] - 3s 18ms/step - loss: 0.5801 - acc: 0.8056 - val_loss: 1.1574 - val_acc: 0.6727\n",
      "Epoch 5/10\n",
      "149/149 [==============================] - 3s 19ms/step - loss: 0.3704 - acc: 0.8849 - val_loss: 1.2094 - val_acc: 0.7041\n",
      "Epoch 6/10\n",
      "149/149 [==============================] - 3s 18ms/step - loss: 0.2555 - acc: 0.9251 - val_loss: 1.2084 - val_acc: 0.6992\n",
      "Epoch 7/10\n",
      "149/149 [==============================] - 3s 19ms/step - loss: 0.1717 - acc: 0.9485 - val_loss: 1.4802 - val_acc: 0.6953\n",
      "Epoch 8/10\n",
      "149/149 [==============================] - 3s 19ms/step - loss: 0.1201 - acc: 0.9657 - val_loss: 1.4654 - val_acc: 0.7012\n",
      "Epoch 9/10\n",
      "149/149 [==============================] - 3s 18ms/step - loss: 0.0946 - acc: 0.9758 - val_loss: 1.6993 - val_acc: 0.7031\n",
      "Epoch 10/10\n",
      "149/149 [==============================] - 3s 19ms/step - loss: 0.0663 - acc: 0.9840 - val_loss: 1.7519 - val_acc: 0.7007\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss= 'categorical_crossentropy',optimizer= 'rmsprop',metrics = ['acc'])\n",
    "with tf.device('/CPU:0'):\n",
    "    history = model.fit(data1_train_X_padding, data1_train_y_1hot,epochs = 10, batch_size =32, validation_split= 0.3,verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7e51899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data1\n",
    "# sequencing\n",
    "data1_test_X_sequencing = tokenizer.texts_to_sequences(data1_test_X_morphed)\n",
    "# padding\n",
    "data1_test_X_padding = pad_sequences(data1_test_X_sequencing, maxlen = maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4dfad907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data1 test_y\n",
    "data1_test_y_labeling= e.transform(data1_test_y)\n",
    "data1_test_y_1hot = to_one_hot(data1_test_y_labeling, class_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c0479e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "213/213 [==============================] - 1s 6ms/step - loss: 0.5427 - acc: 0.9065\n"
     ]
    }
   ],
   "source": [
    "# data1 분류1\n",
    "with tf.device('/CPU:0'):\n",
    "    model.evaluate(data1_test_X_padding, data1_test_y_1hot)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
